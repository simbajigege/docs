"use strict";(self.webpackChunkdyte_docs=self.webpackChunkdyte_docs||[]).push([[33190],{82006:(e,i,o)=>{o.r(i),o.d(i,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=o(74848),n=o(28453);const a={title:"Processing - Middlewares",sidebar_position:1,sidebar_slug:"video-processing-middlewares"},r="Video Processing",s={id:"capabilities/video/processing",title:"Processing - Middlewares",description:"Dyte's SDK supports middlewares which are pluggable functions that can be applied to both audio and video streams in a meeting. This lets you tap in the media to either observe or modify. This guide covers the following.",source:"@site/docs/guides/capabilities/video/processing.mdx",sourceDirName:"capabilities/video",slug:"/capabilities/video/processing",permalink:"/docs/guides/capabilities/video/processing",draft:!1,unlisted:!1,editUrl:"https://github.com/dyte-io/docs/tree/main/docs/guides/capabilities/video/processing.mdx",tags:[],version:"current",lastUpdatedAt:1706782034,formattedLastUpdatedAt:"Feb 1, 2024",sidebarPosition:1,frontMatter:{title:"Processing - Middlewares",sidebar_position:1,sidebar_slug:"video-processing-middlewares"},sidebar:"tutorialSidebar",previous:{title:"Transcriptions - Google STT",permalink:"/docs/guides/capabilities/audio/transcriptionGoogle"},next:{title:"Virtual Background",permalink:"/docs/guides/capabilities/video/add-virtual-background"}},d={},c=[{value:'Create a video middleware <div class="header-tag tag-core">Core</div>',id:"create-a-video-middleware-core",level:2},{value:'Add or remove video middlewares <div class="header-tag tag-core">Core</div>',id:"add-or-remove-video-middlewares-core",level:2}];function l(e){const i={code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,n.R)(),...e.components},{Head:o,TabItem:a,Tabs:r}=i;return o||u("Head",!0),a||u("TabItem",!0),r||u("Tabs",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"video-processing",children:"Video Processing"}),"\n",(0,t.jsx)(i.p,{children:"Dyte's SDK supports middlewares which are pluggable functions that can be applied to both audio and video streams in a meeting. This lets you tap in the media to either observe or modify. This guide covers the following."}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsx)(i.li,{children:"Create an video middleware"}),"\n",(0,t.jsx)(i.li,{children:"Add or remove video middlewares"}),"\n"]}),"\n",(0,t.jsxs)(i.h2,{id:"create-a-video-middleware-core",children:["Create a video middleware ",(0,t.jsx)("div",{class:"header-tag tag-core",children:"Core"})]}),"\n",(0,t.jsx)(r,{groupId:"framework",children:(0,t.jsxs)(a,{value:"js",label:"Javascript",children:[(0,t.jsx)(i.p,{children:"All you need to do is create a function that returns another function which gets called for every frame of the video. This function can be synchronous or asynchronous. Actions that need to be performed just once when the video turns on can be done in the outer function. If a user turns their video off and on again, the outer function is called again as well. Actions that need to be performed on every frame are to be done in the inner function."}),(0,t.jsxs)(i.p,{children:["The inner function is called with the ",(0,t.jsx)(i.strong,{children:"canvas"})," and the ",(0,t.jsx)(i.strong,{children:"context"})," of the video stream. This is the same canvas that is used to render the video stream on the screen. This means that you can perform any operations on the canvas and it will be reflected in realtime. Here is what a middleware function looks like."]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"async function GrayScaleFilter() {\n  console.log('this will be called only once when you turn on the video');\n  // Put your async-await API calls AI/ML model fetch here.\n\n  /*\n\t\tReturned function (having canvas, ctx as params) can be an async function as well.\n\t\tWe support async as well as normal functions.\n\t*/\n  return async (canvas, ctx) => {\n    // This will be called 20 - 30 times per second\n    ctx.filter = 'grayscale(1)';\n    ctx.shadowColor = '#000';\n    ctx.shadowBlur = 20;\n    ctx.lineWidth = 50;\n    ctx.strokeStyle = '#000';\n    ctx.strokeRect(0, 0, canvas.width, canvas.height);\n  };\n}\n\nmeeting.self.addVideoMiddleware(GrayScaleFilter);\n"})})]})}),"\n",(0,t.jsxs)(i.h2,{id:"add-or-remove-video-middlewares-core",children:["Add or remove video middlewares ",(0,t.jsx)("div",{class:"header-tag tag-core",children:"Core"})]}),"\n",(0,t.jsx)(r,{groupId:"framework",children:(0,t.jsxs)(a,{value:"js",label:"Javascript",children:[(0,t.jsxs)(i.p,{children:["Middlewares can be added to a meeting, which is why they are associated with a meeting object. To add a video middleware, you need to call the ",(0,t.jsx)(i.code,{children:"addVideoMiddleware"}),". This method takes in a video middleware function as a parameter, creating this middleware object was covered in the previous section. To remove a video middleware, you need to call the ",(0,t.jsx)(i.code,{children:"removeVideoMiddleware"})," method. This method also takes in the video middleware function as a parameter."]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"meeting.self.addVideoMiddleware(YourMiddleware);\n\n// once done, in a later section remove the middleware\nmeeting.self.removeVideoMiddleware(YourMiddleware);\n"})}),(0,t.jsx)(i.p,{children:"If you'd instead like to perform operations on the video stream through your own implementation, you can also programatically access the meeting's video stream. This is accessed from the meeting object."}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"//Somewhere in your codebase\nconst meeting = await DyteClient.init(...);\n\n// You can get the video track which everybody will see if your camera is turned on\nmeeting.self.videoTrack()\n\n// You can also get raw video track\nmeeting.self.rawVideoTrack();\n"})})]})}),"\n",(0,t.jsx)(o,{children:(0,t.jsx)("title",{children:"Processing - Middlewares Guide"})})]})}function h(e={}){const{wrapper:i}={...(0,n.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}function u(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},28453:(e,i,o)=>{o.d(i,{R:()=>r,x:()=>s});var t=o(96540);const n={},a=t.createContext(n);function r(e){const i=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);