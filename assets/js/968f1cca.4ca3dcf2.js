"use strict";(self.webpackChunkdyte_docs=self.webpackChunkdyte_docs||[]).push([[26660],{73955:(e,i,o)=>{o.r(i),o.d(i,{assets:()=>s,contentTitle:()=>d,default:()=>u,frontMatter:()=>r,metadata:()=>n,toc:()=>c});var t=o(74848),a=o(28453);const r={title:"Processing - Middlewares",sidebar_position:1,sidebar_slug:"audio-processing-middlewares"},d="Audio Processing",n={id:"capabilities/audio/processing",title:"Processing - Middlewares",description:"Dyte's SDK supports middlewares which are pluggable functions that can be applied to both audio and video streams in a meeting. This lets you tap in the media to either observe or modify. This guide covers the following.",source:"@site/docs/guides/capabilities/audio/processing.mdx",sourceDirName:"capabilities/audio",slug:"/capabilities/audio/processing",permalink:"/docs/guides/capabilities/audio/processing",draft:!1,unlisted:!1,editUrl:"https://github.com/dyte-io/docs/tree/main/docs/guides/capabilities/audio/processing.mdx",tags:[],version:"current",lastUpdatedAt:1706782034,formattedLastUpdatedAt:"Feb 1, 2024",sidebarPosition:1,frontMatter:{title:"Processing - Middlewares",sidebar_position:1,sidebar_slug:"audio-processing-middlewares"},sidebar:"tutorialSidebar",previous:{title:"Start Building",permalink:"/docs/guides/realtime-chat/build-in-app-chat-exp"},next:{title:"Transcriptions",permalink:"/docs/guides/capabilities/audio/transcriptions"}},s={},c=[{value:'Create an audio middleware <div class="header-tag tag-core">Core</div>',id:"create-an-audio-middleware-core",level:2},{value:'Add or remove audio middlewares <div class="header-tag tag-core">Core</div>',id:"add-or-remove-audio-middlewares-core",level:2}];function l(e){const i={admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components},{Head:o,TabItem:r,Tabs:d}=i;return o||h("Head",!0),r||h("TabItem",!0),d||h("Tabs",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"audio-processing",children:"Audio Processing"}),"\n",(0,t.jsx)(i.p,{children:"Dyte's SDK supports middlewares which are pluggable functions that can be applied to both audio and video streams in a meeting. This lets you tap in the media to either observe or modify. This guide covers the following."}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsx)(i.li,{children:"Create an audio middleware"}),"\n",(0,t.jsx)(i.li,{children:"Add or remove audio middlewares"}),"\n"]}),"\n",(0,t.jsxs)(i.h2,{id:"create-an-audio-middleware-core",children:["Create an audio middleware ",(0,t.jsx)("div",{class:"header-tag tag-core",children:"Core"})]}),"\n",(0,t.jsx)(d,{groupId:"framework",children:(0,t.jsxs)(r,{value:"js",label:"Javascript",children:[(0,t.jsxs)(i.p,{children:["Audio can be altered using ",(0,t.jsx)(i.strong,{children:"ScriptProcessor"})," and ",(0,t.jsx)(i.strong,{children:"AudioWorkletProcessor"}),". We support both ways of performing audio alterations. The middleware function has access to the ",(0,t.jsx)(i.code,{children:"audioContext"})," which can help the developer perform operations on the audio track. Audio middleware function can be synchronous or asychronous and is expected to return a ",(0,t.jsx)(i.code,{children:"AudioWorkletNode"})," or ",(0,t.jsx)(i.code,{children:"ScriptProcessorNode"}),". Here is a sample audio middleware function for your reference:"]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"//Somewhere in your codebase\nconst meeting = await DyteClient.init(...);\n\nasync function addWhiteNoise(audioContext) {\n    const moduleScript = `\n        class WhiteNoiseProcessor extends AudioWorkletProcessor {\n            process (inputs, outputs, parameters) {\n                const output = outputs[0]\n                output.forEach(channel => {\n                    for (let i = 0; i <TabItem channel.length; i++) {\n                        channel[i] = Math.random() * 1.0 - 0.5\n                    }\n                })\n                return true\n            }\n        }\n\n        registerProcessor('white-noise-processor', WhiteNoiseProcessor);\n    `;\n\n    const scriptUrl = URL.createObjectURL(new Blob([moduleScript], { type: 'text/javascript' }));\n    await audioContext.audioWorklet.addModule(scriptUrl);\n\n    const whiteNoise = new AudioWorkletNode(audioContext, 'white-noise-processor');\n    return whiteNoise;\n}\n\n// Add audio middleware\nmeeting.self.addAudioMiddleware(addWhiteNoise);\n"})})]})}),"\n",(0,t.jsxs)(i.h2,{id:"add-or-remove-audio-middlewares-core",children:["Add or remove audio middlewares ",(0,t.jsx)("div",{class:"header-tag tag-core",children:"Core"})]}),"\n",(0,t.jsx)(d,{groupId:"framework",children:(0,t.jsxs)(r,{value:"js",label:"Javascript",children:[(0,t.jsxs)(i.p,{children:["Middlewares can be added to a meeting, which is why they are associated with a meeting object. To add a audio middleware, you need to call the ",(0,t.jsx)(i.code,{children:"addAudioMiddleware"}),". This method takes in a audio middleware function as a parameter, creating this middleware object was covered in the previous section. To remove a audio middleware, you need to call the ",(0,t.jsx)(i.code,{children:"removeAudioMiddleware"})," method. This method also takes in the audio middleware function as a parameter."]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"meeting.self.addAudioMiddleware(YourMiddlewareFunction);\n\n// once done, in a later section remove the middleware\nmeeting.self.removeAudioMiddleware(YourMiddlewareFunction);\n"})}),(0,t.jsx)(i.admonition,{title:"Note",type:"info",children:(0,t.jsx)(i.p,{children:"In case you are building an Audio Transcriptions middleware or any sort of middleware that doesn\u2019t alter the original audio stream, remember to feed the output channel with whatever you get from inputChannel so that the audio can be sent to the next middleware. Otherwise, a blank audio buffer will be sent to the next middleware."})}),(0,t.jsx)(i.p,{children:"If you'd instead like to perform operations on the audio stream through your own implementation, you can also programatically access the meeting's audio stream. This is accessed from the meeting object."}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-jsx",children:"//Somewhere in your codebase\nconst meeting = await DyteClient.init(...);\n\n// You can get the final track which everybody will hear if your microphone is turned on after all the middlewares are applied\nmeeting.self.audioTrack()\n\n// You can also get raw audio track which is the raw microphone track before any middlewares are applied to it\nmeeting.self.rawAudioTrack();\n"})})]})}),"\n",(0,t.jsx)(o,{children:(0,t.jsx)("title",{children:"Processing - Middlewares Guide"})})]})}function u(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}function h(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},28453:(e,i,o)=>{o.d(i,{R:()=>d,x:()=>n});var t=o(96540);const a={},r=t.createContext(a);function d(e){const i=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function n(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:d(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);